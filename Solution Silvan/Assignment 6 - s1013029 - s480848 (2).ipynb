{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laura Tigchelaar (s1013029) - Jelmer Jansen (s4480848)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages. This notebook will only run properly when utils.py from Assignment 1 is in the same folder, since it needs the get_MNIST() function and the RandomIterator class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import Link, Chain, ChainList, report, iterators, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from utils import *\n",
    "from scipy import misc\n",
    "import glob\n",
    "from chainer import iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a generator neural network which can generate MNIST data of class zero when given random noise as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(Chain):\n",
    "    def __init__(self, n_units):\n",
    "        super(Generator, self).__init__()\n",
    "        self.mnist_dim = 28\n",
    "        self.n_units = n_units\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.l1 = L.Linear(n_units * self.mnist_dim **2)  # n_in -> n_units        INPUT LAYER\n",
    "            self.l2 = L.BatchNormalization(n_units * self.mnist_dim ** 2)    # n_units -> n_out       BATCH NORMALIZATION\n",
    "            self.l3 = L.Deconvolution2D(in_channels=n_units, out_channels=1, ksize=3, stride=1,pad=1,outsize=(28,28))                   #  DECONVOLUTION\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = self.l1(x)\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        h = F.reshape(h2, [-1,self.n_units,self.mnist_dim,self.mnist_dim])\n",
    "        y = F.sigmoid(self.l3(h))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a discriminator neural network which outputs 1 when given real MNIST data and 0 when given fake data created by the generator network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(Chain):\n",
    "    def __init__(self, n_units):\n",
    "        super(Discriminator, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.l2 = L.Convolution2D(in_channels=None, out_channels=n_units,ksize=3,stride=1)  # n_units -> n_units  CONVOLUTIONAL LAYER\n",
    "            self.l3 = L.Linear(None, 1)    # n_units -> n_out\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h2 = F.relu(self.l2(x))\n",
    "        y = F.squeeze(self.l3(h2))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some helper functions to make the above networks run properly. The function **randomsample()** will return an array of random numbers of _size_ times _batch_size_. The **plot_loss()** function plots the loss of both networks, which is given by the input as a double list. The **showImages()** function will print 10 MNIST images created by the generator network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomsample(size,batch_size):\n",
    "    return np.random.uniform(-1.0,1.0,[batch_size,size]).astype('float32')\n",
    "\n",
    "def plot_loss(loss,epoch):\n",
    "    plt.plot(np.array(range(1, epoch + 1)), np.array(loss[0]), label='Discriminator Loss')\n",
    "    plt.plot(np.array(range(1, epoch + 1)), np.array(loss[1]), label='Generator Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def showImages(gen):\n",
    "    f,axes = plt.subplots(2,5)\n",
    "    noise = randomsample(20, 10)\n",
    "    with chainer.using_config('train', False):\n",
    "        images = gen(noise)\n",
    "\n",
    "    for i in range(0,10):\n",
    "        if(i % 2 == 0):\n",
    "            x = 0\n",
    "        else:\n",
    "            x = 1\n",
    "        y = int(round(i/2,0))\n",
    "\n",
    "        axes[x][y].imshow(np.reshape(images[i].data[:,], (28, 28), order='F'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function which runs both networks for a certain number of epochs. For each epoch, this function takes _k_ batches from the data and lets the discriminator compare these to batches of data created by the generator. The loss is then calculated with a sigmoid cross entropy function, and used to update the weights of the discriminator network through the _backward()_ and _update()_ functions. \n",
    "\n",
    "Then the generator network is updated by taking the probability that its images get recognized as fake data as the loss, which is then used to update the weights of the generator network through the _backward()_ and _update()_ functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_network(epoch,batch_size,gen,dis,iterator,n_units,g_optimizer,d_optimizer):\n",
    "    losses = [[],[]]\n",
    "    for i in range(0,epoch):\n",
    "        #for j in range (0,batch_size) THEY USED K=1 IN THE PAPER SO SO DO WE\n",
    "        dloss_all = 0\n",
    "        gloss_all = 0\n",
    "        with chainer.using_config('train', True):\n",
    "\n",
    "            for batch in iterator:\n",
    "                dis.cleargrads(); gen.cleargrads()\n",
    "                noise = randomsample(20,batch_size)\n",
    "                g_sample = gen(noise)\n",
    "                disc_gen = dis(g_sample)\n",
    "                disc_data = dis(np.reshape(batch, (batch_size, 1, 28, 28), order='F'))\n",
    "                softmax1 = F.sigmoid_cross_entropy(disc_gen,np.zeros(batch_size).astype('int32'))\n",
    "                softmax2 = F.sigmoid_cross_entropy(disc_data,np.ones(batch_size).astype('int32'))\n",
    "                loss = softmax1 + softmax2\n",
    "                loss.backward()\n",
    "                d_optimizer.update()\n",
    "\n",
    "                noise = randomsample(20, batch_size)\n",
    "                gn = gen(noise)\n",
    "                gloss = F.sigmoid_cross_entropy(disc_gen,np.ones(batch_size).astype('int32'))\n",
    "                gloss.backward()\n",
    "                g_optimizer.update()\n",
    "\n",
    "                dloss_all +=gloss.data\n",
    "                gloss_all +=loss.data\n",
    "            losses[0].append(dloss_all)\n",
    "            losses[1].append(gloss_all)\n",
    "            print(i)\n",
    "    return losses\n",
    "\n",
    "def getCats():\n",
    "    images = []\n",
    "    for image_path in glob.glob(\"/home/jelmer/Github/CCN_Project/cropped_catfaces/*.jpg\"):\n",
    "        image = misc.imread(image_path)\n",
    "        images.append(image)\n",
    "        #print image.shape\n",
    "    return images\n",
    "\n",
    "def grayscale(images):\n",
    "    output = []\n",
    "    for i in images:\n",
    "        r, g, b = i[:, :, 0], i[:, :, 1], i[:, :, 2]\n",
    "        gray = np.float32(0.2989) * r + np.float32(0.5870) * g + np.float32(0.1140) * b\n",
    "        output.append(gray)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameters of the network, including the number of epochs, the batch size, and the number of hidden units per network layer. Also define the two networks themselves, an iterator for the train data and optimizers for both the networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 20\n",
    "#train_data, test_data = get_mnist(n_train=1000,n_test=100,with_label=False,classes=[0], n_dim=3)\n",
    "cats = getCats()\n",
    "cats = grayscale(cats)\n",
    "train_data = cats[0:1000]\n",
    "#showtrain(train_data)\n",
    "test_data = cats[1000:1100]\n",
    "batch_size = 5\n",
    "n_units = 10\n",
    "gen = Generator(n_units)\n",
    "dis = Discriminator(n_units)\n",
    "iterator = iterators.SerialIterator(train_data, batch_size=batch_size)\n",
    "g_optimizer = optimizers.Adam()\n",
    "g_optimizer.setup(gen)\n",
    "d_optimizer = optimizers.Adam()\n",
    "d_optimizer.setup(dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run both adversarial networks, plot the loss and show 10 images created by the generator network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b41867b2b7da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-b76c9dbb87f5>\u001b[0m in \u001b[0;36mrun_network\u001b[0;34m(epoch, batch_size, gen, dis, iterator, n_units, g_optimizer, d_optimizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mdis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleargrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleargrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mg_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mdisc_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mdisc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-3490aedda726>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jelmer/Downloads/ENTER/lib/python2.7/site-packages/chainer/links/normalization/batch_normalization.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m             func = batch_normalization.BatchNormalizationFunction(\n\u001b[1;32m    142\u001b[0m                 self.eps, self.avg_mean, self.avg_var, decay)\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_mean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jelmer/Downloads/ENTER/lib/python2.7/site-packages/chainer/function.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_indexes_to_retain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_indexes_to_retain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitervalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jelmer/Downloads/ENTER/lib/python2.7/site-packages/chainer/functions/normalization/batch_normalization.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_xhat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpander\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jelmer/Downloads/ENTER/lib/python2.7/site-packages/chainer/functions/normalization/batch_normalization.pyc\u001b[0m in \u001b[0;36m_xhat\u001b[0;34m(x, mean, std, expander)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_xhat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpander\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mx_mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexpander\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mx_mu\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexpander\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "loss = run_network(epoch,batch_size,gen,dis,iterator,n_units,g_optimizer,d_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showImages(gen)\n",
    "plot_loss(loss,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "i = 0\n",
    "dataset = []\n",
    "for file in os.listdir('C:/Users/Laura/Documents/GitHub/CCN_Project/cropped_catfaces'):\n",
    "    dataset.append(file)\n",
    "    if i > 10:\n",
    "        break        \n",
    "    i+=1\n",
    "print dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}